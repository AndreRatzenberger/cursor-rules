---
description: Automatic test creation, execution and management for AI-driven development
globs: 
alwaysApply: true
---

# Automated Testing Management

Rule for automatically creating, executing, and managing tests throughout the development lifecycle.

<rule>
name: automated_testing
filters:
  - type: event
    pattern: "implementation_start"
  - type: event
    pattern: "implementation_complete"
  - type: command
    pattern: "test"
  - type: file_change
    pattern: "src/*"
  - type: file_change
    pattern: "tests/*"
  - type: event
    pattern: "test_failure"

actions:
  - type: react
    event: "implementation_start"
    action: |
      # Create test files when implementation begins
      
      When implementation starts, I'll:
      
      1. Find the active task and its associated specifications
      2. For each specification, create appropriate test files:
         - Detect the project type (Node.js, Python, Rust, etc.)
         - Create test files in the proper location and format
         - Link tests to the specification requirements
         - Add placeholder tests for each requirement
         - Include references to the specs and task
      
      3. Consider project-specific testing patterns:
         - Use Jest for JavaScript/TypeScript
         - Use pytest for Python
         - Use Cargo test for Rust
         - Use JUnit for Java
      
      This ensures tests are created before implementation, supporting 
      test-driven development.

  - type: react
    conditions:
      - pattern: "test run|test execute"
    action: |
      # Run tests based on project type
      
      I'll execute tests for the project:
      
      1. Detect the project type:
         - Node.js (package.json) → npm test
         - Rust (Cargo.toml) → cargo test
         - Java (pom.xml) → mvn test
         - Python (requirements.txt/setup.py) → pytest or unittest
      
      2. Process test results:
         - If tests pass, update active task to reflect passing tests
         - If tests fail, create detailed failure report
         - Trigger appropriate success/failure events
      
      3. Update task acceptance criteria:
         - Mark "Unit tests pass" criteria based on results
      
      This validates implementation quality through automated testing.

  - type: react
    event: "test_failure"
    action: |
      # Analyze test failures and suggest fixes
      
      When tests fail, I'll:
      
      1. Analyze the failure details:
         - Identify failing tests
         - Determine failure reasons
         - Isolate problematic code
      
      2. Create a learning about the test failure:
         - Document the failure details
         - Record potential solutions
         - Link to relevant code and specifications
      
      3. Store the learning for future reference:
         - Save to `.cursor/learnings/` with proper ID
         - Update the learnings index
      
      This captures valuable information from failures and helps prevent 
      similar issues in the future.

  - type: react
    event: "implementation_complete"
    action: |
      # Run verification tests when implementation is complete
      
      When implementation is marked complete, I'll:
      
      1. Run all relevant tests:
         - Execute tests based on project type
         - Verify all tests pass
      
      2. Update the task status:
         - Mark acceptance criteria as met if tests pass
         - Indicate readiness for completion
         - Flag issues if tests fail
      
      3. Prevent completion if tests fail:
         - Record verification failure
         - Request fixes before allowing completion
      
      This ensures only properly tested implementations are marked complete.

  - type: react
    event: "file_change"
    conditions:
      - pattern: "src/.*\\.(js|ts|jsx|tsx|py|rs|go|java|rb|cpp|c|h|hpp)$"
    action: |
      # Check for test files when source files change
      
      When a source file changes, I'll:
      
      1. Extract the component name from the file path
      2. Look for corresponding test files in common locations:
         - tests/[component].test.js
         - tests/test_[component].py
         - tests/[component]_test.rs
         - __tests__/[component].test.js
         - etc.
      
      3. If no test file exists:
         - Make note of missing test coverage
         - Create a reminder to add tests
         - Track untested components
      
      This ensures all components have corresponding test coverage.

  - type: suggest
    message: |
      ### Automated Testing Framework

      Tests are automatically managed throughout development:

      **Automatic Behaviors:**
      - When implementation starts → Test files are created based on specs
      - When source files change → Test coverage is checked
      - When implementation completes → Verification tests are run
      - When tests fail → Analysis and learnings are created

      **Commands:**
      - `test run` - Execute tests for the project

      **Integration with Tasks:**
      - Tasks track test status in acceptance criteria
      - Tasks require passing tests for completion
      - Test failures are recorded as learnings

      This framework ensures comprehensive test coverage and prevents 
      untested code from being marked complete.

examples:
  - input: |
      # When implementation starts
      Implementation starting for user authentication feature
    output: "Test files created for user authentication based on specifications"

  - input: |
      test run
    output: "Running tests... ✅ Tests passed successfully! Task updated to reflect passing tests."

  - input: |
      # When implementation is complete
      Implementation complete for user authentication
    output: "Running verification tests... ✅ Implementation verified by tests. Task ready for completion."

metadata:
  priority: high
  version: 1.0
</rule>
